# AI Medical Assistant Web App

An intelligent medical diagnosis platform powered by LangGraph workflows and local AI models, featuring real-time symptom analysis, image classification, and comprehensive healthcare recommendations.

## ğŸŒŸ Features

### **Multi-Modal Medical Analysis**
- **Textual Symptom Analysis**: Advanced LLM-powered diagnosis from symptom descriptions
- **Medical Image Classification**: Skin lesion analysis using EfficientNet
- **Follow-up Interactions**: Dynamic questioning system for comprehensive diagnosis
- **Overall Analysis**: Synthesized insights from all available data sources

### **AI-Powered Chatbot Workflow**
- **LangGraph Integration**: Sophisticated agent orchestration and workflow management
- **Local Model Support**: Privacy-focused local LLM execution (Llama 3.1, BioMistral)
- **Real-time Processing**: WebSocket-based live updates and streaming responses
- **Confidence Scoring**: Quantified diagnostic confidence with threshold-based routing

### **Smart Healthcare Recommendations**
- **Specialist Referrals**: Intelligent matching with appropriate medical specialists
- **Urgency Assessment**: Automated severity classification and emergency detection
- **Self-care Guidance**: Personalized health management recommendations
- **Comprehensive Reports**: Detailed medical analysis summaries

## ğŸš€ Quick Start
### Prerequisites
- **Python 3.8+** with pip
- **Node.js 16+** with npm
- **Git** with Git LFS for version control and model downloads
- **4GB+ RAM** for local AI models
- **Hugging Face account** (optional but recommended for model downloads)

### 1. Clone & Setup
```bash
git clone https://github.com/Jwongjs/AI-Medical-Assistant-Web-App.git
cd AI-Medical-Assistant-Web-App
```

### 2. Backend Setup
```bash
cd backend

# Install Python dependencies
pip install -r requirements.txt
```
### 3. Model Configuration

**Required AI Models:**
The application requires the following AI models to function properly:

1) **Main Language Model**: `Llama-3.1-8B-UltraMedical.Q8_0.gguf`
   - **Source**: Hugging Face - https://huggingface.co/mradermacher/Llama-3.1-8B-UltraMedical-GGUF
   - **How to Get**: Direct download from HF

2) **Embedding Model**: `sentence-transformers/all-MiniLM-L6-v2`
   - **Note**: Downloaded automatically via Python when first used

3) **Image Classification Model**:
- skin_lesion_efficientnetb0.pth (should be included within the folder, else download model, phase2_best.pth within the google drive https://drive.google.com/file/d/15LBP6awUDjMOQFftsVC1GxA6kpi-4Yj0/view?usp=sharing)

**After Downloading:**
Place the following models within
```bash
backend/ai_models/Llama-3.1-8B-UltraMedical.Q8_0.gguf
```

### 4. Start both end servers

#### - Backend Setup
```bash
python main.py
```

#### - Frontend Setup
```bash
cd my-app

# Install Node dependencies
npm install

# Start development server
npm start
```

### - Access Application
- **Frontend**: http://localhost:3000
- **Backend API**: http://localhost:8000
- **API Docs**: http://localhost:8000/docs


## ğŸ“‚ Project Structure

```
AI-Medical-Assistant-Web-App/
â”œâ”€â”€ ğŸ“ backend/                    # Python FastAPI backend
â”‚   â”œâ”€â”€ ğŸ“ adapters/              # Model abstraction layer
â”‚   â”‚   â”œâ”€â”€ base.py               # Base model interface
â”‚   â”‚   â”œâ”€â”€ local_model_adapter.py # Local LLM integration
â”‚   â”‚   â”œâ”€â”€ hf_api_adapter.py     # Hugging Face API client
â”‚   â”‚   â””â”€â”€ skinlesion_efficientNet_adapter.py # Image classifier
â”‚   â”œâ”€â”€ ğŸ“ api/                   # REST API endpoints
â”‚   â”œâ”€â”€ ğŸ“ graphs/                # LangGraph workflow definitions
â”‚   â”œâ”€â”€ ğŸ“ managers/              # State & model management
â”‚   â”œâ”€â”€ ğŸ“ nodes/                 # Workflow node implementations
â”‚   â”œâ”€â”€ ğŸ“ schemas/               # Pydantic data models
â”‚   â”œâ”€â”€ ğŸ“ ai_models/             # Local AI model files
â”‚   â”œâ”€â”€ main.py                   # FastAPI application entry
â”‚   â””â”€â”€ requirements.txt          # Python dependencies
â”œâ”€â”€ ğŸ“ my-app/                    # React TypeScript frontend
â”‚   â”œâ”€â”€ ğŸ“ src/
â”‚   â”‚   â”œâ”€â”€ ğŸ“ components/        # Reusable UI components
â”‚   â”‚   â”œâ”€â”€ ğŸ“ pages/             # Main application pages
â”‚   â”‚   â”œâ”€â”€ ğŸ“ hooks/             # Custom React hooks
â”‚   â”‚   â”œâ”€â”€ ğŸ“ services/          # API integration layer
â”‚   â”‚   â”œâ”€â”€ ğŸ“ workflow/          # Workflow routing logic
â”‚   â”‚   â””â”€â”€ ğŸ“ types/             # TypeScript definitions
â”‚   â”œâ”€â”€ package.json              # Node.js dependencies
â”‚   â””â”€â”€ tsconfig.json             # TypeScript configuration
â”œâ”€â”€ ğŸ“ 4_deployment/              # Docker & Kubernetes configs
â”œâ”€â”€ start-local-test.bat          # Windows startup script
â””â”€â”€ README.md                     # This file
```

## ğŸ”„ Workflow Stages

The application follows a sophisticated multi-stage workflow:

1. **Textual Analysis** â†’ Initial symptom processing
2. **Follow-up Questions** â†’ Dynamic clarification (optional)
3. **Image Analysis** â†’ Medical image classification (optional)
4. **Overall Analysis** â†’ Comprehensive data synthesis
5. **Medical Report** â†’ Final comprehensive summary

## ğŸ› ï¸ Development

### Key Technologies
- **Backend**: FastAPI, LangGraph, LangChain, Pydantic, llama-cpp-python
- **Frontend**: React 19, TypeScript, Custom CSS
- **AI Models**: Llama 3.1, BioMistral, EfficientNet
- **Deployment**: Docker, Kubernetes (optional)

### Design Principles
1. **Lazy Loading**: Minimal LLM output optimization
2. **Singleton Pattern**: Efficient model sharing
3. **Privacy-First**: Local model execution
4. **Real-time Updates**: WebSocket streaming

### Adding New Models
1. Implement the `ModelInterface` in `backend/adapters/base.py`
2. Create adapter class inheriting from `ModelInterface`
3. Register in `backend/managers/model_manager.py`
4. Update workflow nodes as needed

## ğŸ§ª Testing

### Backend Tests
```bash
cd backend
python quick_test.py        # Local model testing
python hf_api_test.py         # Hugging Face integration
```

### Frontend Tests
```bash
cd my-app
npm test                      # Jest test suite
npm run build                 # Production build test
```

## ğŸš€ Deployment

### Local Development
```bash
# Windows
./start-local-test.bat

# Linux/Mac
./start-local-test.sh
```

<!-- ### Docker Deployment
```bash
cd 4_deployment/docker
docker-compose up -d
```

### Kubernetes Deployment
```bash
cd 4_deployment/kubernetes
kubectl apply -f medical-ai-deployment.yaml
``` -->

## ğŸ“Š Performance & Resources

### System Requirements
- **RAM**: 8GB minimum (16GB recommended for larger models)
- **Storage**: 10GB+ for AI models
- **CPU**: Multi-core processor recommended
- **GPU**: Optional (CUDA support for faster inference)

### Model Performance
- **Llama 3.1-8B**: ~4GB VRAM, 2-5 tokens/sec
- **EfficientNet**: ~1GB VRAM, <1sec inference
- **Embedding Model**: ~500MB RAM, <100ms
---
